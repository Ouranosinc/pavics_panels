{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "saved-beverage",
   "metadata": {},
   "source": [
    "# Visualizing Effective Climate Sensitivity \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-separate",
   "metadata": {
    "iooxa": {
     "id": {
      "block": "FuSC6vVrd6Lnmt6ikf6L",
      "project": "ggYBMgwL5hEPpQHWxS28",
      "version": 2
     },
     "outputId": null
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_zelinka():\n",
    "    \"\"\"Read CMIP6 Effective Climate Sensitivity (ECS) from Zelinka.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xr.DataFrame\n",
    "      Effective Climate Sensitivity indexed by model name.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    Zelinka, M. D., Myers, T. A., McCoy, D. T., Po-Chedley, S., Caldwell, P. M., Ceppi, P., et al. (2020). \n",
    "    Causes of higher climate sensitivity in CMIP6 models. Geophysical Research Letters, 47, e2019GL085782. \n",
    "    https://doi.org/10.1029/2019GL085782\n",
    "    \n",
    "    Links to data: \n",
    "    \n",
    "      - https://github.com/mzelinka/cmip56_forcing_feedback_ecs\n",
    "      - https://zenodo.org/record/6647291#.Y4Eb39LMJhE\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    with open(\"data/cmip56_forcing_feedback_ecs.json\", \"r\") as f:\n",
    "        doc = json.load(f)\n",
    "    c6 = doc[\"CMIP6\"]\n",
    "\n",
    "    ecs = {}\n",
    "    for (model, values) in c6.items():\n",
    "        # Take the first realization from the list\n",
    "        ecs[model] = list(values.values())[0][\"ECS\"]\n",
    "\n",
    "        \n",
    "    ecs = pd.Series(ecs).to_frame(\"ECS\")\n",
    "    \n",
    "    return ecs\n",
    "\n",
    "\n",
    "def write_zelinka():\n",
    "    \"\"\"Write Zelinka ECS estimates to disk.\"\"\"\n",
    "    ecs = extract_zelinka()\n",
    "    ecs.to_json(\"zelinka_ecs.json\")\n",
    "    \n",
    "    \n",
    "def load_zelinka():\n",
    "    \"\"\"Load Zelinka ECS data.\"\"\"\n",
    "    out = pd.read_json(\"zelinka_ecs.json\")\n",
    "    out.index.name = \"Model\"\n",
    "    return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_zelinka()\n",
    "ecs = load_zelinka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_sherwood():\n",
    "    \"\"\"Extract posterior probability density function (pdf) from Sherwood's supplementary material.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xr.DataFrame\n",
    "      Posterior pdf for the effective climate sensitivity.\n",
    "    \n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    Webb, M. (2020). Code and Data for WCRP Climate Sensitivity Assessment. \n",
    "    https://doi.org/10.5281/ZENODO.3945276 \n",
    "    \n",
    "    \"\"\"\n",
    "    from joblib import load\n",
    "    \n",
    "    def _kernel_smooth(bin_centres,y,bin_width, n_bins):\n",
    "\n",
    "      # apply Gausian Kernel smoothing to y\n",
    "      # JDA uses sd of 0.1\n",
    "      kernel_sd = 0.1\n",
    "\n",
    "      smoothed_y=np.copy(y)\n",
    "\n",
    "      # apply kernel filter over central range\n",
    "      istart=int(n_bins/2-4000)\n",
    "      iend=int(n_bins/2+4000)\n",
    "\n",
    "      smooth=True\n",
    "      if smooth:\n",
    "        for i in range(istart,iend):\n",
    "          x=bin_centres[i]\n",
    "          k = np.exp(-1*( x - bin_centres ) ** 2 / (2 * kernel_sd ** 2))\n",
    "          smoothed_y[i] = np.sum(y * k)\n",
    "          #print ('i=',i,'bin_center=',bin_centres[i],x,y[i],smoothed_y[i])\n",
    "\n",
    "      smoothed_y=_normalise_pdf(smoothed_y,bin_width)\n",
    "\n",
    "      return(smoothed_y)\n",
    "\n",
    "    def _normalise_pdf(pdf,bin_width):\n",
    "      return(pdf/np.sum(pdf, dtype=np.float64)/bin_width)\n",
    "\n",
    "    \n",
    "    # Load data\n",
    "    inpath = \"/home/david/projects/HQ/avis_scenarios_2140/data/WCRP_ECS_assessment_code_200714\"\n",
    "    calc_id = \"ULI_MEDIUM_SAMPLE\"\n",
    "    dumpfile = inpath + '/' + calc_id + '/' + calc_id + '.lastmean.joblib'\n",
    "    [transfer_unweighted_prior_pdf, transfer_weighted_prior_pdf, total_hist_erf_posterior, total_hist_erf_prior,\n",
    "     ecs_pdf, posterior, n_bins, bin_boundaries, bin_centres, bin_width, n_samples, s_pdf, s_prior_pdf,\n",
    "     full_l_prior_pdf, l_process_bu_likelihood, l_process_ec_likelihood, l_hist_likelihood, l_paleo_cold_likelihood,\n",
    "     l_paleo_hot_likelihood, l_prior, l_posterior, s_process_bu_likelihood, s_process_ec_likelihood, s_hist_likelihood,\n",
    "     s_paleo_cold_likelihood, s_paleo_hot_likelihood, full_s_prior_pdf] = load(dumpfile)\n",
    "\n",
    "    x = np.array(bin_centres)\n",
    "    i = (x < 8) * (x > 0)\n",
    "    \n",
    "    # Normalize and smooth posterior\n",
    "    posterior = _normalise_pdf(posterior, bin_width)\n",
    "    smoothed_posterior=_kernel_smooth(bin_centres,posterior,bin_width, n_bins)\n",
    "    cdf = np.cumsum(smoothed_posterior) * bin_width\n",
    "    \n",
    "    out = pd.DataFrame({\"ECS\": x[i], \"pdf\": smoothed_posterior[i], \"cdf\": cdf[i]})\n",
    "    return out\n",
    "    \n",
    "    \n",
    "def write_sherwood():\n",
    "    df = extract_sherwood()\n",
    "    df.to_json(\"sherwood_ecs.json\")\n",
    "    \n",
    "    \n",
    "def load_sherwood():\n",
    "    out = pd.read_json(\"sherwood_ecs.json\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_sherwood()\n",
    "df = load_sherwood()\n",
    "#print(df.reindex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "    \n",
    "def graph_baseline_ecs_pdf():\n",
    "    \"\"\"Plot Baseline posterior from Sherwood's paper.\"\"\"\n",
    "\n",
    "    df = pd.read_json(\"sherwood_ecs.json\").reindex()\n",
    "    pdf = df[\"pdf\"]\n",
    "    x = df[\"ECS\"]\n",
    "    cdf = df[\"cdf\"]\n",
    "    \n",
    "    ac = \"#36494f\"\n",
    "    ac2 = \"orange\"\n",
    "    with plt.rc_context(\n",
    "            {'axes.edgecolor': ac, 'axes.labelcolor': ac, 'xtick.color': ac, 'ytick.color': ac, 'figure.facecolor':\n",
    "                'white'}):\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6.5, 3), dpi=300)\n",
    "        fig.subplots_adjust(bottom=.15)\n",
    "        l1 = ax.plot(x, pdf, color=\"k\", label=\"Densité de probabilité\")\n",
    "    \n",
    "    with plt.rc_context(\n",
    "            {'axes.labelcolor': ac, 'xtick.color': ac, 'ytick.color': ac2, }):\n",
    "        ax2 = ax.twinx()\n",
    "        l2 = ax2.plot(x, 1-cdf, color=ac2, label=\"Probabilité de dépassement\", clip_on=False)\n",
    "    \n",
    "    lns = l1+l2\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    #ax.legend(lns, labs, frameon=False)\n",
    "\n",
    "    ax.set_xlim([0,8])\n",
    "    ax.set_ylim([0, .8])\n",
    "    ax2.set_xlim([0, 8])\n",
    "    ax2.set_ylim([0, 1])\n",
    "        \n",
    "    for axi in [ax, ax2]:\n",
    "        for key, spine in axi.spines.items():\n",
    "            if key in [\"top\"]:\n",
    "                spine.set_visible(False)\n",
    "                \n",
    "    ax.set_xlabel(\"Sensibilité climatique effective (K)\")\n",
    "    ax.set_ylabel(\"Densité de probabilité (K$^{-1}$)\")\n",
    "    ax2.set_ylabel(\"Probabilité de dépassement\")\n",
    "    \n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-cheese",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "Créer une figure interactive qui permet de sélectionner un ensemble de modèle en fonction de la sensibilité climatique. \n",
    "\n",
    "- ~~Créer une figure qui montre le pdf de la sensibilité climatique~~\n",
    "- ~~Ajouter des points qui représentent la valeur d'ECS des modèles~~\n",
    "- ~~Ajouter un sélecteur permettant de choisir une plage de valeurs d'ECS - colorer les modèles sélectionnés~~\n",
    "- ~~Afficher la probabilité que l'ECS se trouve dans la plage choisie~~\n",
    "- ~~Bouton copié-collé pour copier un json des modèles choisis et de leur ECS. ~~\n",
    "- ~~Ajouter des boutons pour les valeurs de ECS communs~~\n",
    "- ~~Calculer un poids pour passer du pdf empirique au pdf de Sherwood~~\n",
    "    - ~~Updater le poids lorsque la selection change~~\n",
    "    - ~~updater le poids dans le df/json~~\n",
    "- ~~ajouter un toggle pour montrer le pdf weighted ou non~~\n",
    "- ~~mieux cacher le dataframe~~\n",
    "- ~~aligner les elements sur le panel.~~\n",
    "- ~~Show model selection if it changes via buttons~~\n",
    "- ~~Turn the dataframes into a class for easier interop (i.e. don't have to re-calculate the weights for 3 different functions)~~\n",
    "\n",
    "# Bonus\n",
    "- Faire un graphique similaire pour TCRE, et faire un \"linked selection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eastern-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"# Context\n",
    "\n",
    "The CMIP6 ensemble has what is called a *hot model* problem ([Hausfather et al. (2022)](https://doi.org/10.1038/d41586-022-01192-2)). That is, many models have a high climate sensitivity, and taking an unweighted average of temperature changes from the model ensemble would yield a warming higher than best estimates based on multiple historical and paleoclimate observations. This dashboard presents various strategies to weigh CMIP6 models based on climate sensivity estimates. \n",
    "\n",
    "## Measures of climate sensitivity\n",
    "\n",
    "Climate sensitivity is usually defined as the global temperature increase following a doubling of CO2 concentration in the atmosphere, compared to pre-industrial levels (~260 ppm).\n",
    "\n",
    "\n",
    "Transient Climate Response\n",
    ": The temperature change at the moment that atmospheric CO2 has doubled in a scenario where CO2 increases at a rate of 1% each year (about 70 years).\n",
    "\n",
    "Equilibrium Climate Sensitivity\n",
    ": The temperature change once the climate has fully adjusted to a doubling of atmospheric CO2, so after thousands oy years to account for the slow response of oceans.\n",
    "\n",
    "Effective Climate Sensitivity\n",
    ": An approximation of the equilibrium climate sensitivity found by analysing the first 150 years of an abrupt 2xCO2 or 4xCO2 simulation, assuming linear climate feedbacks.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-background",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "The CMIP6 ensemble has what is called a *hot model* problem ([Hausfather et al. (2022)](https://doi.org/10.1038/d41586-022-01192-2)). That is, many models have a high climate sensitivity, and taking an unweighted average of temperature changes from the model ensemble would yield a warming higher than best estimates based on multiple historical and paleoclimate observations. This dashboard presents various strategies to weigh CMIP6 models based on climate sensivity estimates. \n",
    "\n",
    "## Measures of climate sensitivity\n",
    "\n",
    "Climate sensitivity is usually defined as the global temperature increase following a doubling of CO2 concentration in the atmosphere, compared to pre-industrial levels (~260 ppm).\n",
    "\n",
    "\n",
    "Transient Climate Response\n",
    ": The temperature change at the moment that atmospheric CO2 has doubled in a scenario where CO2 increases at a rate of 1% each year (about 70 years).\n",
    "\n",
    "Equilibrium Climate Sensitivity\n",
    ": The temperature change once the climate has fully adjusted to a doubling of atmospheric CO2, so after thousands oy years to account for the slow response of oceans.\n",
    "\n",
    "Effective Climate Sensitivity\n",
    ": An approximation of the equilibrium climate sensitivity found by analysing the first 150 years of an abrupt 2xCO2 or 4xCO2 simulation, assuming linear climate feedbacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = \"\"\"# Data sources\n",
    "\n",
    "CMIP-6 model ECS\n",
    ": Mark Zelinka. (2022). mzelinka/cmip56_forcing_feedback_ecs: Jun 15, 2022 Release (v2.2). Zenodo. https://doi.org/10.5281/zenodo.6647291\n",
    "\n",
    "Expected ECS\n",
    ": Sherwood, S. C., Webb, M. J., Annan, J. D., Armour, K. C., Forster, P. M., Hargreaves, J. C., et al. (2020). An assessment of Earth's climate sensitivity using multiple lines of evidence. Reviews of Geophysics, 58, e2019RG000678. https://doi.org/10.1029/2019RG000678\n",
    "\n",
    "Hausfather Likely/Very likely range\n",
    ": Hausfather, Z., Marvel, K., Schmidt, G. A., Nielsen-Gammon, J. W., & Zelinka, M. (2022). Climate simulations: Recognize the ‘hot model’problem. https://doi.org/10.1038/d41586-022-01192-2\n",
    "\n",
    "IPCC likely/very likely range\n",
    ": IPCC, 2021: Summary for Policymakers. In: Climate Change 2021: The Physical Science Basis. Contribution of Working Group I to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change [Masson-Delmotte, V., P. Zhai, A. Pirani, S.L. Connors, C. Péan, S. Berger, N. Caud, Y. Chen, L. Goldfarb, M.I. Gomis, M. Huang, K. Leitzell, E. Lonnoy, J.B.R. Matthews, T.K. Maycock, T. Waterfield, O. Yelekçi, R. Yu, and B. Zhou (eds.)]. In Press.\n",
    "\n",
    "\n",
    "## Note\n",
    "\n",
    "The IPCC and Hausfather use Equilibrium Climate Sensitivity, which is not available for all models. We multiply these ranges by a factor of {np.round(100.0/(1.0+model_obj.adjustment_factor),2)}% to obtain the Effective Climate Sensitivity, as per Sherwood et al, although we neglect the uncertainty in this adjustment factor. Alternatively, you can take account of the uncertainty by multiplying by the appropriate distribution. This is done on the 'equilibrium' tab.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-effort",
   "metadata": {},
   "source": [
    "# Data sources\n",
    "\n",
    "CMIP-6 model ECS\n",
    ": Mark Zelinka. (2022). mzelinka/cmip56_forcing_feedback_ecs: Jun 15, 2022 Release (v2.2). Zenodo. https://doi.org/10.5281/zenodo.6647291\n",
    "\n",
    "Expected ECS\n",
    ": Sherwood, S. C., Webb, M. J., Annan, J. D., Armour, K. C., Forster, P. M., Hargreaves, J. C., et al. (2020). An assessment of Earth's climate sensitivity using multiple lines of evidence. Reviews of Geophysics, 58, e2019RG000678. https://doi.org/10.1029/2019RG000678\n",
    "\n",
    "Hausfather Likely/Very likely range\n",
    ": Hausfather, Z., Marvel, K., Schmidt, G. A., Nielsen-Gammon, J. W., & Zelinka, M. (2022). Climate simulations: Recognize the ‘hot model’problem. https://doi.org/10.1038/d41586-022-01192-2\n",
    "\n",
    "IPCC likely/very likely range\n",
    ": IPCC, 2021: Summary for Policymakers. In: Climate Change 2021: The Physical Science Basis. Contribution of Working Group I to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change [Masson-Delmotte, V., P. Zhai, A. Pirani, S.L. Connors, C. Péan, S. Berger, N. Caud, Y. Chen, L. Goldfarb, M.I. Gomis, M. Huang, K. Leitzell, E. Lonnoy, J.B.R. Matthews, T.K. Maycock, T. Waterfield, O. Yelekçi, R. Yu, and B. Zhou (eds.)]. In Press.\n",
    "\n",
    "\n",
    "## Note\n",
    "\n",
    "The IPCC and Hausfather use Equilibrium Climate Sensitivity, which is not available for all models. We multiply these ranges by a factor of {np.round(100.0/(1.0+model_obj.adjustment_factor),2)}% to obtain the Effective Climate Sensitivity, as per Sherwood et al, although we neglect the uncertainty in this adjustment factor. Alternatively, you can take account of the uncertainty by multiplying by the appropriate distribution. This is done on the 'equilibrium' tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425de561-7dc2-4488-a561-e9e205de4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = load_sherwood().reset_index()\n",
    "models = load_zelinka().reset_index()\n",
    "models.sort_values(\"ECS\", inplace=True)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import colorcet as cc\n",
    "import hvplot.xarray # gives hvplot method to pandas objects\n",
    "import hvplot.pandas\n",
    "from scipy import stats\n",
    "from scipy import interpolate\n",
    "from scipy import integrate\n",
    "import numpy as np\n",
    "hv.extension('bokeh')\n",
    "from bokeh.models import tools\n",
    "import panel as pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e64c6-9add-4395-92f7-0074f35f4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an generator for the kernel density estimate for the sherwood models:\n",
    "#\n",
    "#kde_stats = stats.gaussian_kde(models.ECS, bw_method = BW_METHOD)\n",
    "## get the density estimate, and add it to the models df:\n",
    "#models['density'] = kde_stats.pdf(models.ECS)\n",
    "## get the density estimate for all ECSs in pdf.ECS, using the generator for models. \n",
    "#kde_data = pd.DataFrame({'ECS':pdf.ECS,'PDF':kde_stats.pdf(pdf.ECS)})\n",
    "## get the transform to go from the sherwood PDF to the Zelinka pdf:\n",
    "#kde_data['weights'] = pdf.pdf / kde_data.PDF\n",
    "## get a generator to interpolate these weights:\n",
    "#weightfunc = interpolate.interp1d(kde_data.ECS,kde_data.weights)\n",
    "## get the weights for each model:\n",
    "#models['weight'] = weightfunc(models.ECS)\n",
    "## get the new_density for each model (equivalent to running gaussian_kde with weights=models.weight):\n",
    "#models['new_density'] = models.density * models.weight\n",
    "def quantile_1D(data, weights, quantile):\n",
    "    \"\"\"\n",
    "    Compute the weighted quantile of a 1D numpy array. from https://github.com/nudomarinero/wquantiles/blob/master/wquantiles.py\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ndarray\n",
    "        Input array (one dimension).\n",
    "    weights : ndarray\n",
    "        Array with the weights of the same size of `data`.\n",
    "    quantile : float\n",
    "        Quantile to compute. It must have a value between 0 and 1.\n",
    "    Returns\n",
    "    -------\n",
    "    quantile_1D : float\n",
    "        The output value.\n",
    "    \"\"\"\n",
    "    # Check the data\n",
    "    if not isinstance(data, np.matrix):\n",
    "        data = np.asarray(data)\n",
    "    if not isinstance(weights, np.matrix):\n",
    "        weights = np.asarray(weights)\n",
    "    nd = data.ndim\n",
    "    if nd != 1:\n",
    "        raise TypeError(\"data must be a one dimensional array\")\n",
    "    ndw = weights.ndim\n",
    "    if ndw != 1:\n",
    "        raise TypeError(\"weights must be a one dimensional array\")\n",
    "    if data.shape != weights.shape:\n",
    "        raise TypeError(\"the length of data and weights must be the same\")\n",
    "    if ((quantile > 1.) or (quantile < 0.)):\n",
    "        raise ValueError(\"quantile must have a value between 0. and 1.\")\n",
    "    # Sort the data\n",
    "    ind_sorted = np.argsort(data)\n",
    "    sorted_data = data[ind_sorted]\n",
    "    sorted_weights = weights[ind_sorted]\n",
    "    # Compute the auxiliary arrays\n",
    "    Sn = np.cumsum(sorted_weights)\n",
    "    # TODO: Check that the weights do not sum zero\n",
    "    #assert Sn != 0, \"The sum of the weights must not be zero\"\n",
    "    Pn = (Sn-0.5*sorted_weights)/Sn[-1]\n",
    "    # Get the value of the weighted median\n",
    "    return np.interp(quantile, Pn, sorted_data)\n",
    "\n",
    "class ModelData():\n",
    "    ''' class to hold model data\n",
    "    '''\n",
    "    models   = None # for model data\n",
    "    sherwood = None # for sherwood data\n",
    "    filtered = None # for filtered model data\n",
    "    interp   = None # for interpolated filtered data to sherwood ECS\n",
    "    x_sel    = [0,10.0] # filter to apply.\n",
    "    is_updated = False\n",
    "    weighted   = False\n",
    "    bw_method = None\n",
    "    adjustment_factor = 0.06\n",
    "    adjustment_scale  = 0.2\n",
    "    def get_eq(self,obj):\n",
    "        ecs_x = obj.ECS\n",
    "        pdf_x = obj.PDF\n",
    "        y = ecs_x[1:]#np.linspace(0.0,10.0,100)\n",
    "        interpolator_x = interpolate.interp1d(ecs_x,pdf_x,bounds_error=False,fill_value=0)\n",
    "        interpolator_y = lambda x : stats.norm.pdf(x,loc=(1.+self.adjustment_factor),scale=self.adjustment_scale)\n",
    "        interpolator_xy = lambda t : integrate.trapz(x=y,y=(1/(y) * interpolator_y(t / y) * interpolator_x(y))) # -inf to inf ideally, 0 to 8 is good enough.\n",
    "        return [interpolator_xy(z) for z in ecs_x]\n",
    "    def __init__(self,models,sherwood,lazy=True,weighted = False, bw_method='scott'):\n",
    "        self.models = models\n",
    "        self.sherwood  = sherwood\n",
    "        self.lazy = lazy\n",
    "        self.bw_method = bw_method\n",
    "        self.weighted  = weighted\n",
    "        self.sherwood['PDFeq'] = self.get_eq(self.sherwood)\n",
    "        \n",
    "    def set_filter(self,x_sel):\n",
    "        if not all([self.x_sel[i] == x_sel[i] for i,x in enumerate(x_sel)]):\n",
    "            self.is_updated = False\n",
    "        self.x_sel = x_sel\n",
    "        if (not self.lazy) and (not self.is_updated):\n",
    "            self.apply_filter()\n",
    "    \n",
    "    def apply_filter(self):\n",
    "        self.filtered = self.models[(self.models.ECS > self.x_sel[0]) & (self.models.ECS < self.x_sel[1])].copy()\n",
    "        self.update_kde()\n",
    "        self.is_updated = True\n",
    "    \n",
    "    def my_kde(self,data,bw,weights=[]):\n",
    "        kde = np.zeros(self.sherwood.ECS.shape)\n",
    "        mean_kernel = integrate.trapz(x=self.sherwood.ECS,y=self.sherwood.ECS * self.sherwood.PDF)\n",
    "        std_kernel    = np.sqrt(integrate.trapz(x=self.sherwood.ECS,y=(self.sherwood.ECS ** 2.0) * self.sherwood.PDF) - (mean_kernel ** 2))\n",
    "        centered_kernel = interpolate.interp1d((self.sherwood.ECS - mean_kernel) / std_kernel, self.sherwood.PDF * std_kernel,bounds_error=False,fill_value=0)\n",
    "        n = len(data) if len(weights) != len(data) else np.sum(weights)**2 / np.sum(weights**2)\n",
    "        if type(bw) == str:\n",
    "            if bw == 'scott':\n",
    "                bw = n ** (-1.0 / 5.)\n",
    "            elif bw == 'silverman':\n",
    "                bw = (n * (1. + 2.) / 4.)**(-1. / (1. + 4.))\n",
    "                \n",
    "        for i in data.index:\n",
    "            if len(weights) == len(data):\n",
    "                weight = weights[i]\n",
    "            else:\n",
    "                weight = n ** (-1)\n",
    "            centre = data[i]\n",
    "            kde += weight * (1./bw) * centered_kernel((self.sherwood.ECS - centre) / bw)\n",
    "        return kde\n",
    " \n",
    "    def update_kde(self,update_weights=True):\n",
    "        if self.weighted and (type(self.filtered) != type(None)) and ('weight' in self.filtered.columns) and not update_weights:\n",
    "            kde_stats = self.my_kde(self.filtered.ECS,bw = self.bw_method,weights=self.filtered.weight)#stats.gaussian_kde(self.filtered.ECS,, bw_method = self.bw_method)\n",
    "        else:\n",
    "            kde_stats = self.my_kde(self.filtered.ECS,bw = self.bw_method,weights=np.array([]))\n",
    "            #kde_stats = stats.gaussian_kde(self.filtered.ECS,bw_method = self.bw_method)\n",
    "        self.interp = pd.DataFrame({'ECS':self.sherwood.ECS})\n",
    "        \n",
    "        interpolator = interpolate.interp1d(self.sherwood.ECS,kde_stats,bounds_error=False,fill_value=0)\n",
    "        self.filtered['PDF'] = interpolator(self.filtered.ECS)\n",
    "        self.interp['PDF'] = interpolator(self.interp.ECS)\n",
    "        \n",
    "        if update_weights:\n",
    "            self.interp['weight'] = self.sherwood.PDF / self.interp.PDF\n",
    "            weightfunc = interpolate.interp1d(self.interp.ECS,self.interp.weight)\n",
    "            self.filtered['weight'] = weightfunc(self.filtered.ECS)\n",
    "            norm_weight_factor = 1.0 / np.sum(self.filtered.weight)\n",
    "            self.filtered['weight'] *= norm_weight_factor\n",
    "            if self.weighted:\n",
    "                self.update_kde(update_weights=False)\n",
    "    \n",
    "    def map_quantiles(self,x_in,x_out,from_pdf,to_pdf):\n",
    "        from_cdf = np.cumsum(from_pdf)\n",
    "        to_cdf   = np.cumsum(to_pdf)\n",
    "        \n",
    "        func_to_cdf = interpolate.interp1d(x_in,to_cdf,kind='cubic',bounds_error=False)\n",
    "        func_from_cdf = interpolate.interp1d(x_in,from_cdf, kind='cubic',bounds_error=False)\n",
    "        \n",
    "        quantiles_from = func_from_cdf(x_out)\n",
    "        quantiles_to   = func_to_cdf(x_out)\n",
    "        weights = quantiles_to / (quantiles_from + np.finfo(float).eps)\n",
    "        return weights\n",
    "    \n",
    "    def set_weighted(self,weighted):\n",
    "        if self.weighted != weighted:\n",
    "            self.is_updated = False\n",
    "        self.weighted = weighted\n",
    "        if not self.lazy:\n",
    "            self.apply_filter()\n",
    "    \n",
    "    def iter_kde(self,max_iter = 10):\n",
    "        if not self.weighted:\n",
    "            self.update_kde()\n",
    "            return\n",
    "        for i in range(max_iter):\n",
    "            self.update_kde()\n",
    "        \n",
    "    def get_filtered(self):\n",
    "        if not self.is_updated:\n",
    "            self.apply_filter()\n",
    "        return self.filtered\n",
    "    \n",
    "    def get_interp(self):\n",
    "        if not self.is_updated:\n",
    "            self.apply_filter()\n",
    "        return self.interp\n",
    "    \n",
    "    def get_median(self,dataset,weights):\n",
    "        return quantile_1D(dataset,weights,0.5)\n",
    "    \n",
    "    def get_summarystats(self):\n",
    "        self.apply_filter()\n",
    "        summary = pd.DataFrame({\n",
    "                        (\"all, sherwood\"):       {\"mean\":0, 'median':0,'std':0},\n",
    "                        (\"selection, sherwood\"): {\"mean\":0, 'median':0,'std':0},\n",
    "                        (\"selection, models\"):   {\"mean\":0, 'median':0,'std':0},\n",
    "                       })\n",
    "        sherwood_filter = (self.sherwood.ECS > self.x_sel[0]) & (self.sherwood.ECS < self.x_sel[1])\n",
    "        dataset = self.sherwood\n",
    "        summary.loc['mean',  ('all, sherwood')] = integrate.trapz(x=dataset.ECS,y=dataset.ECS * dataset.PDF)\n",
    "        summary.loc['median',('all, sherwood')] = self.get_median(dataset.ECS,dataset.PDF)\n",
    "        summary.loc['std',   ('all, sherwood')] = np.sqrt(integrate.trapz(x=dataset.ECS,y=(dataset.ECS ** 2.0) * dataset.PDF) - (summary.loc['mean',  ('all, sherwood')] ** 2))\n",
    "\n",
    "        dataset = self.sherwood[sherwood_filter]\n",
    "        summary.loc['mean',  ('selection, sherwood')] = integrate.trapz(x=dataset.ECS,y=dataset.ECS * dataset.PDF)\n",
    "        summary.loc['median',('selection, sherwood')] = self.get_median(dataset.ECS,dataset.PDF)\n",
    "        summary.loc['std',   ('selection, sherwood')] = np.sqrt(integrate.trapz(x=dataset.ECS,y=(dataset.ECS ** 2.0) * dataset.PDF) - (summary.loc['mean',  ('selection, sherwood')] ** 2))\n",
    "        if self.weighted:\n",
    "            summary.loc['mean',  ('selection, models')] = (self.filtered.ECS * self.filtered.weight).sum()\n",
    "            summary.loc['median',('selection, models')] = self.get_median(self.filtered.ECS,self.filtered.weight)\n",
    "            summary.loc['std',   ('selection, models')] = ((self.filtered.ECS - summary.loc['mean',  ('selection, models')])**2 * self.filtered.weight).sum()\n",
    "        else:\n",
    "            summary.loc['mean',  ('selection, models')] = self.filtered.ECS.mean()\n",
    "            summary.loc['median',('selection, models')] = self.filtered.ECS.median()\n",
    "            summary.loc['std',   ('selection, models')] = self.filtered.ECS.std()\n",
    "        return summary\n",
    "    \n",
    "    def get_prob(self, distr='sherwood'):\n",
    "        ''' returns the probability for the ECS to be within the x_sel bounds.\n",
    "        '''\n",
    "        dataset = None\n",
    "        if distr.lower() == 'sherwood':\n",
    "            sherwood_filter = (self.sherwood.ECS >= self.x_sel[0]) & (self.sherwood.ECS <= self.x_sel[1])\n",
    "            dataset = self.sherwood[sherwood_filter]\n",
    "        elif (distr.lower() == 'models') or (distr.lower() == 'interp'):\n",
    "            if not self.is_updated:\n",
    "                self.apply_filter()\n",
    "            dataset = self.interp\n",
    "        else:\n",
    "            raise ValueError(f'Cannot interpret distr={distr}. use sherwood or models/interp.')\n",
    "        probability = integrate.trapz(x=dataset.ECS,y=dataset.PDF)\n",
    "        return probability\n",
    "\n",
    "BW_METHOD = 'scott'\n",
    "model_obj = ModelData(models=models.rename({'density':'PDF'},axis='columns',inplace=False),\n",
    "                      sherwood=pdf.rename({'pdf':'PDF'},axis='columns',inplace=False),\n",
    "                      weighted = False,\n",
    "                      lazy = True,\n",
    "                      bw_method = BW_METHOD)\n",
    "#print(model_obj.get_filtered().head())\n",
    "model_obj.set_filter([0,10])\n",
    "model_obj.set_weighted(True)\n",
    "model_obj.apply_filter()\n",
    "#print(model_obj.get_filtered().head())\n",
    "#model_obj.iter_kde()\n",
    "#print(model_obj.get_filtered().head())\n",
    "\n",
    "def remove_tools(plot, element):\n",
    "    '''remove_tools: removes unneeded (and unwanted) tools from the given bokeh plot.\n",
    "    '''\n",
    "    curr_tools = plot.state.tools\n",
    "    new_tools = []\n",
    "    exclude_list=['LassoSelectTool','PanTool']\n",
    "    for tool in curr_tools:\n",
    "        if not any([(exclude in str(type(tool))) for exclude in exclude_list]):\n",
    "            new_tools.append(tool)\n",
    "    plot.state.tools = new_tools\n",
    "\n",
    "def fix_dims(plot, element):\n",
    "    '''fix_dims: sets given bokeh plot box_select tool to only allow width (x) selections.\n",
    "    '''\n",
    "    dims_list = ['BoxSelectTool']\n",
    "    for tool in plot.state.tools:\n",
    "        if any([(boxtool in str(type(tool))) for boxtool in dims_list]):\n",
    "            tool.dimensions = 'width'\n",
    "            \n",
    "\n",
    "def display_event(data={}):\n",
    "    ''' Event triggered whenever selection changes. \n",
    "    Effects: \n",
    "        1- display new selection bounds\n",
    "        2- display probability of selection\n",
    "        3- display number of models selected\n",
    "        4- updates json of models selected, and displays this\n",
    "        5- updates df of models selected, and displays this.\n",
    "    '''\n",
    "    if data['type'] == 'Effective':\n",
    "        x1str = 'x1_eff'\n",
    "        x2str = 'x2_eff'\n",
    "    else:\n",
    "        x1str = 'x1_eq'\n",
    "        x2str = 'x2_eq'\n",
    "    x_sel = [data[x1str],data[x2str]]\n",
    "    x1 = data[x1str]\n",
    "    x2 = data[x2str]\n",
    "    weighted = data['weighted']\n",
    "    model_obj.set_filter(x_sel=x_sel)\n",
    "    model_obj.set_weighted(weighted=weighted)\n",
    "    df = model_obj.get_filtered()\n",
    "    summary = model_obj.get_summarystats()\n",
    "    \n",
    "    text_widget.object = f\"\"\"ECS bounds: {np.round(x1,2)}°C, {np.round(x2,2)}°C\n",
    "    prob (Sherwood) = {np.round(100 * model_obj.get_prob('sherwood'),2)} %\n",
    "    number of models: {df.shape[0]} / {model_obj.models.shape[0]}\n",
    "    \"\"\"\n",
    "    summary_stats.object = summary\n",
    "    json_widget.object = df.to_json()\n",
    "    df_widget.object = df\n",
    "    \n",
    "def save_notebook(event):\n",
    "    ''' event triggered whenever button \"save to jupyterlab\" is pressed.\n",
    "    Effects:\n",
    "    1- Saves the json to the jupyterlab environment, with filename given in filename_widget.\n",
    "    2- points the button save_widget_computer to the new filename.\n",
    "    '''\n",
    "    # update the filename for save_widget_computer:\n",
    "    save_widget_computer.filename = get_filename()\n",
    "    # dump json to file:\n",
    "    json_str = json_widget.object\n",
    "    with open(get_filename(), 'w') as outfile:\n",
    "        outfile.write(json_str)\n",
    "        \n",
    "def get_filename():\n",
    "    ''' wrapper to return filename_widget.value\n",
    "    '''\n",
    "    return filename_widget.value\n",
    "\n",
    "def make_rectangle_eff(data=[]):\n",
    "    return make_rectangle(data={'x1':data['x1_eff'], 'x2':data['x2_eff']})\n",
    "\n",
    "def make_rectangle_eq(data=[]):\n",
    "    return make_rectangle(data={'x1':data['x1_eq'], 'x2':data['x2_eq']})\n",
    "\n",
    "def make_rectangle(data=[]):\n",
    "    x1str = 'x1'\n",
    "    x2str = 'x2'\n",
    "    rect = hv.Rectangles([(data[x1str],size_rect[1],data[x2str],size_rect[3])])\n",
    "    rect.opts(alpha = 0.1)\n",
    "    if data[x2str] < data[x1str]:\n",
    "        rect.opts(color='red')\n",
    "    else:\n",
    "        rect.opts(color='cyan')\n",
    "    return rect\n",
    "def make_kde_eq(data=[]):\n",
    "    model_obj.set_filter(x_sel=[data['x1_eff'],data['x2_eff']])\n",
    "    model_obj.set_weighted(weighted=data['weighted'])\n",
    "    \n",
    "    ecs = model_obj.get_interp().ECS\n",
    "    pdf = model_obj.get_eq(obj=model_obj.get_interp()) # interpolated PDF of N(mu,sigma)*interp\n",
    "    weighted_str = \"\" if not data['weighted'] else \"Weighted \"\n",
    "    label = f'{weighted_str}Experimental Density (CMIP-6)'\n",
    "    return pd.DataFrame({'ECS':ecs,'PDF':pdf}).hvplot.line(x='ECS',y='PDF',label=label)\n",
    "\n",
    "def make_kde_eq_scatter(data=[]):\n",
    "    model_obj.set_filter(x_sel=[data['x1_eff'],data['x2_eff']])\n",
    "    model_obj.set_weighted(weighted=data['weighted'])\n",
    "    \n",
    "    interp = model_obj.get_interp()\n",
    "    \n",
    "    all_data = model_obj.models\n",
    "    eq = model_obj.get_eq(obj=interp) # interpolated PDF of N(mu,sigma)*interp\n",
    "\n",
    "    mu_eq = (1. + model_obj.adjustment_factor) * all_data.ECS\n",
    "    sigma_eq = all_data.ECS * model_obj.adjustment_scale\n",
    "    \n",
    "    pdf_mu_eq = np.interp(xp=interp.ECS,fp=eq,x=mu_eq)\n",
    "    \n",
    "    weighted_str = \"\" if not data['weighted'] else \"Weighted \"\n",
    "    label = f'{weighted_str}Selected models'\n",
    "    selector = (mu_eq < data['x2_eq']) & (mu_eq > data['x1_eq'])\n",
    "    sel_interp = pd.DataFrame({'ECS':mu_eq[selector], 'PDF':pdf_mu_eq[selector]})\n",
    "    all_interp = pd.DataFrame({'ECS':mu_eq,'PDF':pdf_mu_eq})\n",
    "    label_all = f'{weighted_str}CMIP-6 models'\n",
    "    select_plot = sel_interp.hvplot.scatter(x='ECS',y='PDF',label=label, color = 'red', alpha = 1)\n",
    "    all_plot    = all_interp.hvplot.scatter(x='ECS',y='PDF',label=label_all, color = 'maroon', alpha = 1)\n",
    "    \n",
    "    all_interp['xerr'] = sigma_eq\n",
    "    errors = hv.ErrorBars(data=all_interp,horizontal=True,vdims=['PDF','xerr'],label='Error bars on EffCS to EqCS conversion')\n",
    "    return errors * all_plot * select_plot\n",
    "        \n",
    "def make_kde(data = []):\n",
    "    model_obj.set_filter(x_sel=[data['x1_eff'],data['x2_eff']])\n",
    "    model_obj.set_weighted(weighted=data['weighted'])\n",
    "    \n",
    "    weighted_str = \"\" if not data['weighted'] else \"Weighted \"\n",
    "    label = f'{weighted_str}Experimental Density (CMIP-6)'\n",
    "    return model_obj.get_interp().hvplot.line(x='ECS',y='PDF', label=label)\n",
    "\n",
    "def make_kde_scatter(data = []):\n",
    "    model_obj.set_filter(x_sel=[data['x1_eff'],data['x2_eff']])\n",
    "    model_obj.set_weighted(weighted=data['weighted'])\n",
    "    weighted_str = \"\" if not data['weighted'] else \"Weighted \"\n",
    "    label_selected = f'{weighted_str}selected models'\n",
    "    label_unselected = f'{weighted_str}CMIP-6 models'\n",
    "    scatter_selected = model_obj.get_filtered().hvplot.scatter(x='ECS',y='PDF', hover_cols=['Model','weight'],label = label_selected,color='red', alpha = 1)\n",
    "    df = model_obj.models.copy()\n",
    "    df['PDF'] = np.interp(xp=model_obj.interp.ECS,fp=model_obj.interp.PDF,x=df.ECS)\n",
    "    scatter_unselected = df.hvplot.scatter(x='ECS',y='PDF',label = label_unselected).opts(color = 'maroon', alpha = 1)\n",
    "    \n",
    "    return  scatter_unselected * scatter_selected\n",
    "\n",
    "def convert_eq_to_eff(eq):\n",
    "    return eq / (1. + model_obj.adjustment_factor)\n",
    "\n",
    "def convert_eff_to_eq(eff):\n",
    "    return (1. + model_obj.adjustment_factor) * eff\n",
    "\n",
    "def pipe_wrapper(*args,**kwargs):\n",
    "    curr = pipe.data\n",
    "    if 'type' in kwargs:\n",
    "        if kwargs['type'] == 'Effective':\n",
    "            curr['type'] = 'Effective'\n",
    "        elif kwargs['type'] == 'Equilibrium':\n",
    "            curr['type'] = 'Equilibrium'\n",
    "    if 'x_selection' in kwargs:\n",
    "        if curr['type'] == 'Effective':\n",
    "            curr['x1_eff'] = kwargs['x_selection'][0]\n",
    "            curr['x2_eff'] = kwargs['x_selection'][1]\n",
    "            curr['x1_eq']  = convert_eff_to_eq(curr['x1_eff'])\n",
    "            curr['x2_eq']  = convert_eff_to_eq(curr['x2_eff'])\n",
    "        elif curr['type'] == 'Equilibrium':\n",
    "            curr['x1_eq'] = kwargs['x_selection'][0]\n",
    "            curr['x2_eq'] = kwargs['x_selection'][1]\n",
    "            curr['x1_eff']  = convert_eq_to_eff(curr['x1_eq'])\n",
    "            curr['x2_eff']  = convert_eq_to_eff(curr['x2_eq'])\n",
    "    if 'x_eff' in kwargs:\n",
    "        curr['x1_eff'] = kwargs['x_eff'][0]\n",
    "        curr['x2_eff'] = kwargs['x_eff'][1]\n",
    "        curr['x1_eq']  = convert_eff_to_eq(curr['x1_eff'])\n",
    "        curr['x2_eq']  = convert_eff_to_eq(curr['x2_eff'])\n",
    "    elif 'x_eq' in kwargs:\n",
    "        curr['x1_eq'] = kwargs['x_eq'][0]\n",
    "        curr['x2_eq'] = kwargs['x_eq'][1]\n",
    "        curr['x1_eff']  = convert_eq_to_eff(curr['x1_eq'])\n",
    "        curr['x2_eff']  = convert_eq_to_eff(curr['x2_eq'])\n",
    "    if 'weight' in kwargs:\n",
    "        if kwargs['weight'] == 'Weighted':\n",
    "            curr['weighted'] = True\n",
    "        elif kwargs['weight'] == 'Unweighted':\n",
    "            curr['weighted'] = False\n",
    "    pipe.send(data=curr)\n",
    "    \n",
    "def update_weight(*args):\n",
    "    if args and type(args[0] == hv.param.Event):\n",
    "        if args[0].new == 'Weighted':\n",
    "            pipe_wrapper(weight='Weighted')\n",
    "        if args[0].new == 'Unweighted':\n",
    "            pipe_wrapper(weight='Unweighted')\n",
    "\n",
    "def update_plottype(*args):\n",
    "    if args and type(args[0] == hv.param.Event):\n",
    "        if args[0].new == 0:\n",
    "            pipe_wrapper(type='Effective')\n",
    "        if args[0].new == 1:\n",
    "            pipe_wrapper(type='Equilibrium')\n",
    "            \n",
    "citation_widget = pn.pane.Markdown(context + sources, width=600, extensions=[\"extra\",])\n",
    "\n",
    "# Widget to display selection\n",
    "\n",
    "# widget to pre-select IPCC likely, very likely, hausfather:\n",
    "# note that these are EQUILIBRIUM climate sensitivities. We need to adjust via 1/(1+0.06) [Sherwood et al] to arrive at Effective Climate Sensitivity\n",
    "def get_range_sherwood():\n",
    "    quantiles = [66,90]\n",
    "    ranges = {}\n",
    "    cdf = integrate.cumtrapz(x=model_obj.sherwood.ECS,y=model_obj.sherwood.PDF,initial=0.)\n",
    "    cdf_u, ind_u = np.unique(cdf,return_index=True)\n",
    "    ppf = interpolate.interp1d(x=cdf[ind_u],y=model_obj.sherwood.ECS[ind_u])\n",
    "    for q in quantiles:\n",
    "        left = (1. - (q * 1.0 / 100)) / 2\n",
    "        right = 1 - left\n",
    "        ranges[q] = (ppf(left),ppf(right))\n",
    "    return ranges\n",
    "\n",
    "select_buttons = {\n",
    "    \"IPCC\": {\n",
    "             \"Likely\":      {'range':(2.5,4), 'button':pn.widgets.Button(name = 'Select IPCC AR6 \"Likely\" range') },\n",
    "             \"Very likely\": {'range':(2, 5), 'button':pn.widgets.Button(name = 'Select IPCC AR6 \"Very likely\" range') }\n",
    "            },\n",
    "    \"Hausfather\": {\n",
    "                   \"Likely\":      {'range':(2.6, 3.9), 'button':pn.widgets.Button(name = 'Select Hausfather \"Likely\" range') },\n",
    "                   \"Very likely\": {'range':(2.3, 4.7), 'button':pn.widgets.Button(name = 'Select Hausfather \"Very likely\" range') }\n",
    "                  },\n",
    "    \"Likelyhood\": {\n",
    "                   \"Likely\":      {'range':get_range_sherwood()[66], \n",
    "                                   'button':pn.widgets.Button(name = 'Select Likely (66%) quantiles') },\n",
    "                   \"Very likely\" :{'range':get_range_sherwood()[90], \n",
    "                                   'button':pn.widgets.Button(name = 'Select Very Likely (90%) quantiles') },\n",
    "    },\n",
    "    \"all\" : {\n",
    "              \"definitely\": {'range':( 0, 8), 'button':pn.widgets.Button(name = 'Select all models') } ,\n",
    "            }\n",
    "}\n",
    "preselect_list = pn.Column( name=\"Select models\")\n",
    "_ = [preselect_list.append(select_buttons[name][prob]['button']) for name in select_buttons for prob in select_buttons[name]]\n",
    "\n",
    "text_widget = pn.pane.Str(\"\", width=300, height=50)\n",
    "summary_stats = pn.pane.DataFrame(model_obj.get_summarystats(),width=300)\n",
    "\n",
    "weight_button = pn.widgets.RadioButtonGroup(name=\"Weighted PDF\", options=[\"Unweighted\", \"Weighted\"], button_type=\"primary\")\n",
    "\n",
    "plot_options = pn.Accordion(preselect_list,weight_button, active=[0], sizing_mode='stretch_width')\n",
    "# Widget to display json of models selected:\n",
    "json_widget = pn.pane.JSON(models.to_json(), name='JSON',width = 600)\n",
    "# widget to display dataframe of models selected\n",
    "df_widget   = pn.pane.DataFrame(models,name='DF',width = 600, index = False)\n",
    "# widgets to save json:\n",
    "filename_widget = pn.widgets.TextInput(value='ecs_models.json')\n",
    "save_widget_notebook = pn.widgets.Button(name='Save JSON to JupyterLab',button_type='primary')\n",
    "save_widget_computer = pn.widgets.FileDownload(button_type='success',auto=True, callback = get_filename, filename = get_filename())\n",
    "\n",
    "save_widget_notebook.on_click(save_notebook)\n",
    "\n",
    "# plotting:\n",
    "\n",
    "pipe = hv.streams.Pipe(data={'x1_eff':0,'x2_eff':8,'weighted':False,'type':'Effective'})\n",
    "\n",
    "# plot objects:\n",
    "rect_eff      = hv.DynamicMap(make_rectangle_eff,streams=[pipe]).opts(xlim=(0,8),ylim=(0,1))\n",
    "selection_eff = hv.DynamicMap(make_kde,streams=[pipe], label = 'Density of selection').opts(color='purple',tools=[])\n",
    "scatter_eff   = hv.DynamicMap(make_kde_scatter,streams=[pipe])\n",
    "pdf_plot_eff  = model_obj.sherwood.hvplot.line(x='ECS',y='PDF', color = 'blue', label = 'Expected density (Sherwood)')\n",
    "\n",
    "rect_eq       = hv.DynamicMap(make_rectangle_eq,streams=[pipe]).opts(xlim=(0,8),ylim=(0,1))\n",
    "selection_eq  = hv.DynamicMap(make_kde_eq,streams=[pipe], label = 'Density of selection').opts(color='purple',tools=[])\n",
    "scatter_eq    = hv.DynamicMap(make_kde_eq_scatter,streams=[pipe])\n",
    "#errors_eq     = hv.DynamicMap(make_kde_errors,streams=[pipe]).opts(color='red',muted_alpha = 1)\n",
    "pdf_plot_eq   = model_obj.sherwood.hvplot.line(x='ECS',y='PDFeq', color = 'blue', label = 'Expected density (Sherwood)')\n",
    "\n",
    "pdf_render = hv.render(pdf_plot_eff)\n",
    "size_rect = (pdf_render.x_range.start,pdf_render.y_range.start,pdf_render.x_range.end,pdf_render.y_range.end)\n",
    "pipe.send({'x1_eff':size_rect[0],\n",
    "           'x2_eff':size_rect[2],\n",
    "           'x1_eq':convert_eff_to_eq(size_rect[0]),\n",
    "           'x2_eq':convert_eff_to_eq(size_rect[2]),\n",
    "           'weighted':False,\n",
    "           'type':'Effective'})\n",
    "\n",
    "pdf_plot_eff.opts(tools = ['box_select'])\n",
    "pdf_plot_eq.opts(tools = ['box_select'])\n",
    "\n",
    "pipe.add_subscriber(display_event)\n",
    "weight_button.param.watch(update_weight,'value')\n",
    "\n",
    "sstream_eff = hv.streams.SelectionXY(source=pdf_plot_eff)\n",
    "sstream_eff.add_subscriber(pipe_wrapper)\n",
    "\n",
    "sstream_eq  = hv.streams.SelectionXY(source=pdf_plot_eq)\n",
    "sstream_eq.add_subscriber(pipe_wrapper)\n",
    "def register_func(myrange_eq):\n",
    "    return lambda event : pipe_wrapper(x_eq=myrange_eq)\n",
    "\n",
    "for k,v in select_buttons.items():\n",
    "    for vk,vv in v.items():\n",
    "        vv['func'] = register_func(vv['range'],)\n",
    "        vv['button'].on_click(vv['func'])\n",
    "        \n",
    "# because of a holoviz bug, we can't add rect to all_plot directly (issue #5056)? If all_plot is a dynamicmap plot seems to work...\n",
    "all_plot_eff = rect_eff * pdf_plot_eff * selection_eff * scatter_eff \n",
    "all_plot_eff.opts(xlim = (size_rect[0],size_rect[2]), ylim = (size_rect[1],size_rect[3]))\n",
    "all_plot_eff.opts(legend_position='top_right')\n",
    "all_plot_eff.opts(height=400, show_grid=True)\n",
    "all_plot_eff.opts(default_tools=['save','reset'], tools=[])\n",
    "all_plot_eff.opts(toolbar='above')\n",
    "all_plot_eff.opts(hooks=[remove_tools,fix_dims], active_tools = ['box_select'])\n",
    "all_plot_eff.opts(title='Estimate of Effective Climate Sensitivity for CMIP-6')\n",
    "all_plot_eff.opts(xlabel='Effective Climate Sensitivity (°C)', ylabel='Density (1/°C)')\n",
    "\n",
    "\n",
    "all_plot_eq = rect_eq * pdf_plot_eq * selection_eq *  scatter_eq\n",
    "all_plot_eq.opts(xlim = (size_rect[0],size_rect[2]), ylim = (size_rect[1],size_rect[3]))\n",
    "all_plot_eq.opts(legend_position='top_right')\n",
    "all_plot_eq.opts(height=400, show_grid=True)\n",
    "all_plot_eq.opts(default_tools=['save','reset'], tools=[])\n",
    "all_plot_eq.opts(toolbar='above')\n",
    "all_plot_eq.opts(hooks=[remove_tools,fix_dims], active_tools = ['box_select'])\n",
    "all_plot_eq.opts(title='Estimate of Equilibrium Climate Sensitivity for CMIP-6')\n",
    "all_plot_eq.opts(xlabel='Equilibrium Climate Sensitivity (°C)', ylabel='Density (1/°C)')\n",
    "\n",
    "plot_tabs = pn.Tabs(('Effective',all_plot_eff),('Equilibrium',all_plot_eq))\n",
    "\n",
    "\n",
    "plot_tabs.param.watch(update_plottype,'active')\n",
    "# arrange panel layout:\n",
    "text_widget.height = 100\n",
    "app = pn.Column(pn.Row(plot_tabs,plot_options), \\\n",
    "          pn.Row(text_widget,summary_stats), \\\n",
    "          pn.Row(\n",
    "            pn.Tabs(json_widget,pn.Card(df_widget,title=\"DataFrame\",name=\"DF\",collapsed=True,hide_header=False,sizing_mode='stretch_width')),\n",
    "            pn.Column(filename_widget,\n",
    "                      save_widget_notebook,\n",
    "                      save_widget_computer)\n",
    "                ),\n",
    "         citation_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c461038-f80d-4851-b17e-f54308c23a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2c04a-5249-4bcc-8cdf-c78222d936b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "iooxa": {
   "id": {
    "block": "qMSlX9wzsv3Mu3LWX16m",
    "project": "ggYBMgwL5hEPpQHWxS28",
    "version": 3
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
